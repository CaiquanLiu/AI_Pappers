|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 综述  | NULL |NULL |NULL |NULL |
| 《Pre-trained Models for Natural Language Processing: A Survey》  | Arxiv2020 |邱锡鹏预训练模型综述：<br/>1 Introduction<br/>2 Background<br/>3 Overview of PTMs<br/>4 Extensions of PTMs<br/>5 Adapting PTMs to Downstream Tasks<br/>6 Resources of PTMs<br/>7 Applications<br/>8 Future Directions<br/>9 Conclusion |done |NULL |
| 常规预训练模型  | NULL |NULL |NULL |NULL |
| 《Semi-supervised Sequence Learning》  | NIPS2015 |上下文信息预训练模型最早的论文：<br/> 1 单链LSTM的预训练模型； |done |NULL |
| 《Learned in Translation: Contextualized Word Vectors》  | NIPS2017 |The first paper that proposes a contextualized text representation approach？ |NULL |NULL |
| 《Semi-supervised sequence tagging with bidirectional language models》  | ACL2017 |AllenNLP，TagLM，ELMO的前身：<br/>已经引入了ELMO的双向LSTM预训练向量机制，但重点在序列标注任务; |done |NULL |
| 《Universal Language Model Fine-tuning for Text Classification》  | ACL2018 |FastAI的ULMFiT; <br/>号称ELMO之前的，引入上下文的预训练模型：<br/>三阶段; |done |NULL |
| 《Deep contextualized word representation》  | NAACL2018 |AllenNLP ELMO：非常简单 |done |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| 《Improving Language Understanding by Generative Pre-Training》  | 2018，没发表？ |OpenAI GPT：<br/> 先于BERT提出Pre-train+Fine-tuning范式; |done |NULL |
| 《Language Models are Unsupervised Multitask Learners》  | 2019，没发表？ |GPT-2：<br/>数据集WebText；<br/>Train一个参数超大的语言模型，实现Zero-shot效果；<br/>没有Fine-tuning过程； |done |NULL |
| 《Language Models are Few-Shot Learners》  | NULL |GPT-3：<br/>实现few-shot和one-shot，不需要梯度更新；<br/>可以Fine-tuning，但paper中，没用Fine-tuning；<br/>one-shot和few-shot体现在输入中，而不会用来微调，可以参考paper中的Figure 2.1； |done |NULL |
| 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》  | ACL2019 |BERT |done |哈工大/讯飞联合实验室，中文全词（WWM）：<br/> https://github.com/ymcui/Chinese-BERT-wwm |
| 《Cross-lingual Language Model Pretraining》  | 2019 |Facebook 跨语言XLMs |NULL |NULL |
| 《MASS: Masked Sequence to Sequence Pre-training for Language Generation》  | ICML2019 |微软MASS |NULL |NULL |
| 《Unified Language Model Pre-training for Natural Language Understanding and Generation》  | NULL |微软UniLM1：<br/>XLNet之前，同时结合自回归和自编码思想的预训练模型 |NULL |NULL |
| 《UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training》  | NULL |UniLM2： |NULL |NULL |
| 《MPNet: Masked and Permuted Pre-training for Language Understanding》  | 2020 |MPNet |NULL |NULL |
| 《RoBERTa: A Robustly Optimized BERT Pretraining Approach》  | NULL |RoBERTa |NULL |哈工大/讯飞联合 git地址：<br/> https://github.com/ymcui/Chinese-BERT-wwm |
| 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》  | done |ALBERT:<br/>嵌入层因子分解；<br/>共享参数；<br/>句子连贯性预测； |done |中文ALBERT：https://github.com/brightmart/albert_zh ；<br/>https://www.zhihu.com/question/347898375/answer/863537122 <br/> https://mp.weixin.qq.com/s/LF8TiVccYcm4B6krCOGVTQ |
| 《Pre-Training with Whole Word Masking for Chinese BERT》  | arXiv2019 |主要是引入了WWM（Google官网只有英文的WWM，没有中文的） |done |git地址：<br/> https://github.com/ymcui/Chinese-BERT-wwm |
| 《ERNIE: Enhanced Language Representation with Informative Entities》  | ACL2019 |清华ERNIE：<br/> 输入原始query和query中的实体，并同时预测query中的token和实体 |done |NULL |
| 《ERNIE: Enhanced Representation through Knowledge Integration》  | ArXiv2019 |百度ERNIE 1.0：<br/>和BERT的输入一致，但预测实体和短语（全词MASK） |done |NULL |
| 《ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FORLANGUAGE》  | AAAI2020 |百度ERNIE 2.0：<br/>多任务，持续学习框架 |done |NULL |
| 《ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators》  | ICLR2020 |ELECTRA |NULL |NULL |
| 《K-BERT: Enabling Language Representation with Knowledge Graph》  | 2019 |K-BERT |NULL |NULL |
| 《StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding》  | 2019 |阿里StructBERT |NULL |NULL |
| 《Semantics-aware BERT for Language Understanding》  | AAAI2020 |NULL |NULL |NULL |
| 长文本  | NULL |NULL |NULL |NULL |
| 《XLNet: Generalized Autoregressive Pretraining for Language Understanding》  | NIPS2019 |XLNet：<br/>没怎么看懂，看了很多博客，但还是不太清楚 |done |哈工大/讯飞联合实验室，开源中文XLNet预训练模型：<br> https://github.com/ymcui/Chinese-XLNet |
| 《Longformer: The Long-Document Transformer》| arXiv2020|长文本处理Transformer预训练模型：<br/>1 滑动膨胀Attention；<br/>2 全局Attention;<br/>3 CUDA实现；<br/>4 在RoBert的参数基础上，进行进一步预训练（基于Autoregressive语言模型）；|done|https://zhuanlan.zhihu.com/p/223430086|
| 《Big Bird: Transformers for Longer Sequences》  | 2020 |Big Bird |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| 《Well-Read Students Learn Better: On the Importance of Pre-training Compact Models》  | arXiv2019 |NULL |NULL |Google官方蒸馏方法：<br/> github：https://github.com/google-research/bert |
| 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》  | 2019 |ALBERT |NULL |NULL |
| 《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》  | NIPS2019 |DistilBERT |NULL |NULL |
| 《TinyBERT: Distilling BERT for Natural Language Understanding》  | EMNLP2020 |TinyBERT |NULL |NULL |
| 《FastBERT: a Self-distilling BERT with Adaptive Inference Time》  | ACL2020 |FastBERT |NULL |NULL |
| 超大规模预训练模型  | NULL |NULL |NULL |NULL |
| 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》  | NULL |Google T5：<br>模型结构：Encoder-Decoder、Language model和Prefix LM;<br>注意力掩码机制：Casual、Casual prefix和Fully-visible;<br>预训练方法：语言模型式、Bert Mask式和Deshuffling;<br>Mask策略：Mask模式、Replace span模式和drop模式;<br>Mask比例：10%、15%、25%和50%四种;<br>Span长度：2、3、5和10四种;<br>训练策略：fine-tune、多任务学习、多任务fine-tune和Scaling; |done |博客参考：<br/> https://zhuanlan.zhihu.com/p/88727133 <br/> https://zhuanlan.zhihu.com/p/138235834|
| 《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》  | 2019 |英伟达 Megatron-LM |NULL |NULL |
| 《THE COST OF TRAINING NLP MODELS A CONCISE OVERVIEW》  | NULL |预训练模型成本对比： |done |NULL |
| 特定任务领域预训练模型  | NULL |NULL |NULL |NULL |
| 《SpanBERT: Improving Pre-training by Representing and Predicting Spans》  | TACL2020 |SpanBERT |NULL |NULL |
| 《Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks》  | EMNLP2019 |Sentence-BERT |NULL |NULL |
| 中文预训练模型  | NULL |NULL |NULL |NULL |
| 《Pre-Training with Whole Word Masking for Chinese BERT》  | 2019 |中文WWM预训练BERT |NULL |NULL |
| 《NEZHA: Neural Contextualized Representation for Chinese Language Understanding》  | 2019 |NEZHA |NULL |NULL |
| 《ZEN: Pre-training Chinese (Z) Text Encoder Enhanced by N-gram Representations 》  | 2019 |ZEN |NULL |NULL |
| 《Revisiting Pre-Trained Models for Chinese Natural Language Processing》  | 2020 |MacBERT |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
