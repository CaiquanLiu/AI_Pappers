|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《Semi-supervised Sequence Learning》  | NIPS2015 |上下文信息预训练模型最早的论文 |done |NULL |
| 《Semi-supervised sequence tagging with bidirectional language models》  | ACL2017 |AllenNLP，TagLM，ELMO的前身：<br/>加入语言模型信息; |NULL |NULL |
| 《Universal Language Model Fine-tuning for Text Classification》  | ACL2018 |FastAI的ULMFiT; <br/>号称ELMO之前的，引入上下文的预训练模型：<br/>三阶段; |done |NULL |
| 《Deep contextualized word representation》  | NAACL2018 |AllenNLP ELMO：非常简单 |done |NULL |
| 《Improving Language Understanding by Generative Pre-Training》  | 2018，没发表？ |OpenAI GPT：<br/> 先于BERT提出Pre-train+Fine-tuning范式; |done |NULL |
| 《Language Models are Unsupervised Multitask Learners》  | 2019，没发表？ |GPT-2：<br/>数据集WebText；<br/>Train一个参数超大的语言模型，实现Zero-shot效果；<br/>没有Fine-tuning过程； |done |NULL |
| 《Language Models are Few-Shot Learners》  | NULL |GPT-3：<br/>实现few-shot和one-shot，不需要梯度更新；<br/>可以Fine-tuning，但paper中，没用Fine-tuning；<br/>one-shot和few-shot体现在输入中，而不会用来微调，可以参考paper中的Figure 2.1； |done |NULL |
| 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》  | ACL2019 |BERT |done |哈工大/讯飞联合实验室，中文全词（WWM）：<br/> https://github.com/ymcui/Chinese-BERT-wwm |
| 《Unified Language Model Pre-training for Natural Language Understanding and Generation》  | NULL |微软UniLM1：<br/>XLNet之前，同时结合自回归和自编码思想的预训练模型 |NULL |NULL |
| 《UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training》  | NULL |UniLM2： |NULL |NULL |
| 《XLNet: Generalized Autoregressive Pretraining for Language Understanding》  | NIPS2019 |XLNet：<br/>没怎么看懂，看了很多博客，但还是不太清楚 |done |哈工大/讯飞联合实验室，开源中文XLNet预训练模型：<br> https://github.com/ymcui/Chinese-XLNet |
| 《RoBERTa: A Robustly Optimized BERT Pretraining Approach》  | NULL |RoBERTa |NULL |哈工大/讯飞联合 git地址：<br/> https://github.com/ymcui/Chinese-BERT-wwm |
| 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》  | NULL |ALBERT |NULL |中文ALBERT：<br/> https://github.com/brightmart/albert_zh |
| 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》  | NULL |Google T5：<br>模型结构：Encoder-Decoder、Language model和Prefix LM;<br>注意力掩码机制：Casual、Casual prefix和Fully-visible;<br>预训练方法：语言模型式、Bert Mask式和Deshuffling;<br>Mask策略：Mask模式、Replace span模式和drop模式;<br>Mask比例：10%、15%、25%和50%四种;<br>Span长度：2、3、5和10四种;<br>训练策略：fine-tune、多任务学习、多任务fine-tune和Scaling; |done |博客参考：<br/> https://zhuanlan.zhihu.com/p/88727133 <br/> https://zhuanlan.zhihu.com/p/138235834|
| 《Well-Read Students Learn Better: On the Importance of Pre-training Compact Models》  | arXiv2019 |NULL |NULL |Google官方蒸馏方法：<br/> github：https://github.com/google-research/bert |
| 《Pre-Training with Whole Word Masking for Chinese BERT》  | arXiv2019 |主要是引入了WWM（Google官网只有英文的WWM，没有中文的） |done |git地址：<br/> https://github.com/ymcui/Chinese-BERT-wwm |
| 《ERNIE: Enhanced Language Representation with Informative Entities》  | ACL2019 |清华ERNIE：<br/> 输入原始query和query中的实体，并同时预测query中的token和实体 |done |NULL |
| 《ERNIE: Enhanced Representation through Knowledge Integration》  | ArXiv2019 |百度ERNIE 1.0：<br/>和BERT的输入一致，但预测实体和短语（全词MASK） |done |NULL |
| 《ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FORLANGUAGE》  | AAAI2020 |百度ERNIE 2.0：<br/>多任务，持续学习框架 |done |NULL |
| 《THE COST OF TRAINING NLP MODELS A CONCISE OVERVIEW》  | NULL |预训练模型成本对比： |done |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
