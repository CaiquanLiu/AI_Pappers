|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models》| arxiv2023| 工具使用描述的探索：<br/>1 中心思想就是直接对API定义，比给API的使用示例效果更好；<br/>2 在有API定义的情况下，再做Demo的示例效果不明显；| NULL | NULL |
| 《ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings》| arxiv2023| 解决工具较多场景下的工具使用问题：<br/>1 将工具指定成特殊的Token，然后，在生成阶段预测这些工具（特殊的Token）；<br/>2 主干的大模型是Frozen的，只会微调这些特殊工具Token的嵌入；<br/>3 在数学、问答和任务编排场景进行了验证；<br/>4 看题目以为是用于工具的选择，实际上是直接的工具使用；<br/>5 总体上，感觉可用性并不高。主要是单纯的工具Token嵌入部分的微调，很难真正的理解并记忆工具信息。同时，因为模型不是整体训练的，语义上应该也不是一个整体；| NULL | NULL |
| 《Making Language Models Better Tool Learners with Execution Feedback》| arxiv2023| 基于结果反馈训练语言模型的工具使用能力：<br/>1 基于Alpaca-7B实现；<br/>2 使用了结果反馈信息，但没有用强化学习，但使用了RM模型；<br/>3 基于单工具实现，验证场景是解数学问题（这个场景的反馈奖励好收集）；<br/>4 核心原理是一个请求，同时跑多个模型（ChatGPT、Alpaca、LLaMA等），然后，将不同模型的结果和GoldAnser计算一个归一的Score，最后把不同模型的结果Score的两两对比作为Loss信息；<br/>5 总体上，效果有提升，但也不是很大（对比SFT，3个点以内），而且，工作主要集中在解决数据题上，Score相对好收集，真实的业务场景结果比较多样，Score不一定好收集，复用到其他场景不是很容易；| NULL | NULL |
| 《Android in the Wild: A Large-Scale Dataset for Android Device Control》| arxiv2023| Google+DeepMind联合构造的手机控制数据集AITW：<br/>1 数据集构成：<br/>-用户的prompt（目标）；<br/>-手机的操作：操作动作、坐标、输入内容；<br/>-手机当前的信息：屏幕画面、图标位置和文本信息（OCR）；<br/>2 两个baseline实现：<br/>-基于行为克隆（BC）的实现：多模输入+BERT<br/>-基于大模型：PALM2的ZeroShot和5-shot Chain-of-Though<br/>3 这个工作为后续端到端的多模态操控Android设备做了很好的准备，期待后续工作；| NULL | NULL |
| 《TOOLLLM: FACILITATING LARGE LANGUAGE MODELS TO MASTER 16000+ REAL-WORLD APIS》| arxiv2023| 基于LLM的工具使用方案TOOLLLM：<br/>1 核心工作：<br/>-真实API收集：16464个真实的API（RapidAPI）；<br/>-基于ChatGPT构建用户prompt和solution；<br/>-引入了API检索方案；<br/>-在solution构建和推理步骤引入了DFSDT机制；<br/>2 整体设计和实践非常接近实际可用状态了；| NULL | https://mp.weixin.qq.com/s/-31Em7J-4dDN6a5tc_sGKg |
| 《ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases》| arxiv2023| 中科院软件所的ToolAlpaca：<br/>1 对API做了统一的格式描述；<br/>2 通过ChatGPT扮演三个角色（userAgent、assistantAgent、toolExecutor）来构建训练数据集；<br/>3 基于Alpaca-7B和Alpaca-13B实现；<br/>4 能力覆盖50个大类，400个API；| NULL | NULL |
| 《TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs》| arxiv2023| 微软的大模型扩展能力框架TaskMatrix：<br/>1 核心组件：Multimodal Conversational Foundation Model (MCFM)、API Platform、API Selector、Action Executor；<br/>2 API选择部分没特别介绍，但对API分组是值得借鉴的；<br/>3 整体设计和ToolAlpaca类似，最核心的就一个模块（原始的上下文、候选API作为输入，直接输出接口编排[参数填充+调用顺序]）；| NULL | NULL |
| 《Gorilla: Large Language Model Connected with Massive APIs》| arxiv2023| 能够调用外部工具API的模型Gorilla：<br/>1 基于LLaMA微调实现；<br/>2 基于self-instruct生成训练指令；<br/>3 基于检索使用候选API（候选API噪声过大，会影响结果）；<br/>4 基于AST做测试集验证；<br/>5 对API限制场景做了研究（很少有这方面的研究）；| NULL | NULL |
| 《OpenAGI: When LLM Meets Domain Experts》| arxiv2023| 罗格斯大学的OpenAGI平台：<br/>1 开源了一个致力于大模型使用工具研究的平台；<br/>2 使用CLIP、BERT、ViT自动打分来评估结果；<br/>3 微调了Vicuna-7B和Flan-T5-Large（770M），小参数的T5效果反而更好（怀疑是训练数据少，同时，RLTF在小模型上更容易训练）；<br/>4 引入了RLTF（a Reinforcement Learning from Task Feedback），过程对结果有影响（刚好利用了平台的自动打分能力）；<br/>5 在生成过程中做了限制（缓解错误生成）；<br/>6 主要偏研究，工具是直接提供的，而且，只生成工具的编排，并没有参数的填充；| NULL | NULL |
| 《Reflexion: Language Agents with Verbal Reinforcement Learning》| arxiv2023| Reflexion：通过反思，提升语言模型执行任务的能力：<br/>1 核心模型：<br/>-Actor（LM）：产生行动<br/>-Evaluator（LM）：对行动进行评价<br/>-SelfReflection（LM）：基于Evaluator的评价，给出对Actor的指导建议<br/>2 整个设计中并不包括经典的强化学习，只是类别了Actor和Evaluator；<br/>3 用了三个LM，主要能够更好的应对失败处理。但SelfReflection并不是对每一步的执行实时反馈，需要完成一个完整的Trial后发现失败，然后，再进行SelfReflection，Actor重新规划并执行新的Trial；| NULL | NULL |
| 《LLM Powered Autonomous Agents》| Blog2023| OpenAI研究员Lilian Weng针对LLM Agents的综述：<br/>1 主要针对三个核心方面展开：<br/>-Planning；<br/>-Memory；<br/>-Tool Use； | NULL | https://lilianweng.github.io/posts/2023-06-23-agent/ |
| 《MRKL Systems：A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning》| arxiv2022| AI21使用LLM提升扩展能力的设计：MRKL<br/>1 有效信息不多，主要是将问题分发到其他的处理单元，在分发时会对传参做格式化处理；<br/>2 基于Jurassic-X实现，主要聚焦在简单数学计算的分析上；| NULL | NULL |
| 《TALM: Tool Augmented Language Models》| arxiv2022| 调用外部工具，辅助回答的方案TALM（机构不明，Google？）：<br/>1 基本过程：<br/>-生成使用工具和参数（工具是text-text的）；<br/>-通过BM25检索工具，并使用工具；<br/>-结合工具执行结果，给出最终答案；<br/>2 基于T5（base、large、XL，参数从220M~3B）finetune实现；<br/>3 Self-Play的原理没看懂（如何计算Loss？和强化学习是如何产生联系的？）| NULL | NULL |
|《ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models》| arxiv2023| ReWOO：一种大模型使用工具的优化框架<br/>1 对标ReAct框架，先整体规划执行步骤，然后，再分别调用外部接口，通过调用LLM两次就能获得最终结果；<br/>2 基于LLaMa-7B进行微调，在部分场景效果超过GPT-3.5<br/>3 随着工具变多，效果下降明显；<br/>4 整体的最终结果一般，最好的ACC也只有70%； | NULL | https://mp.weixin.qq.com/s/8cEBOwUyG0zGlC74IuFNeg |
| 《Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models》| arxiv2022| 清华的Open Delta：<br/>1 关于Delta tuning的综述（大多数预训练参数不变，进行少量参数优化）；<br/>2 解决大模型低成本适配下游任务问题；<br/>3 讲方法总结为三类，Addition-based Methods、Specification-based Methods、Reparameterization-based Methods ；| NULL | GitHub - thunlp/OpenDelta: A plug-and-play library for parameter-efficient-tuning (Delta Tuning) |
| 《OpenPrompt: An Open-source Framework for Prompt-learning》 | arxiv2021 | 清华的OpenPrompt：<br/>1 Prompt Learning的工具包；<br/>2 和OpenDelta是一个团队（OpenPrompt更早）。OpenPrompt主要聚焦在prompt上，而OpenDelta主要聚焦在Delta-Tuning（大模型适配层或者中间层优化）； | NULL | https://zhuanlan.zhihu.com/p/607206925 |
| 《Augmented Language Models: a Survey》 | arxiv2023 | ALM：<br/>1 Yann LeCun参与的关于“增强语言模型”的综述;<br/>2 主要聚焦在Reason、Tools、Act；; | NULL | https://mp.weixin.qq.com/s/oCs4R-xYGS42iXIvgnDCCg |
| 《Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback》| arxiv2023| 微软的大模型增强框架：<br/>1 核心组成部分：<br/>-working memory：对话状态记录；<br/>-policy：执行动作生成，通过规则或则模型实现（文中用了基于强化学习的T5）；<br/>-utility：打分或者反馈，通过规则或者模型实现（文中没有特别声明这部分具体的实现）；<br/>-action executor：执行器，调用外部接口；<br/>2 整体设计感觉比较理想，特别是如果是模型实现policy和utility，不管是性能还是效果，感觉都难以保障；| NULL | NULL |
| 《LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS》| arxiv2022|APE方法：使用大模型来生成prompt<br/>1 在实践中可能速度比较慢，成本比较高（要多次调用大模型才能实现）|NULL |NULL |
| 《Learning by Distilling Context》| arxiv2022 | 上下文蒸馏：<br/>1 给大模型更多的输入提示，同时，要求大模型的输出理由和结果；<br/>2 给小模型输入较少的提示，直接输出最终结果；<br/>3 感觉核心就是通过提示让大模型输出的结果更置信，效果肯定没有纯人工构造的数据好（但机器的效率高）；| NULL | NULL |
| 《HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face》| arXiv2023| 基于ChatGPT和HuggingFace模型接口实现多模型中控（基于gpt-3.5-turbo 和 text-davinci-003模型）：<br/>1 核心目标，将用户原始query通过LLM转变成执行的任务，并最终汇总任务结果，产生回复；<br/>2 核心步骤，Task Planning、Model Selection、Task Execution、Response Generation；<br/>3 整体感觉和之前微软用语言模型控制机器人的设计思路是基本一致的；<br/>4 感觉过于理想，应用在工业场景不一定可靠（Limitations部分提到了不稳定，但感觉表述的还是有些轻描淡写了）； | NULL | NULL |
| 《REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS》| ICLR2023| 谷歌的ReAct：使用大模型作为控制中心（主要是基于3~6个few-shot的示例prompt引导后续的模型行为）：<br/>1 主要基于PaLM-540B进行的实验（也部分对比了GPT-3，text-davinci-002）；<br/>2 虽然摘要里提到相比模拟和强化学习的方案有34%和10%的提升，但最的结果也挺一般的；<br/>3 做了基于PaLM-8/62B的Finetuning，这个效果整体感觉还不错；| NULL | NULL |
| 《WebGPT: Browser-assisted question-answering with human feedback》| arxiv2022| OpenAI的WebGPT：<br/>1 基于GPT-3，借助搜索工具，提升模型的问答能力；<br/>2 核心方法：Behavior cloning（BC）、Reward modeling（RM）、Reinforcement learning（RL）、Reject sampling（best-of-n）；<br/>3 生成结果的引用是直接生成的，没有特别的处理(比如xxxx[1]，其中xxx来自文章1)| NULL | NULL |
| 《Tool Learning with Foundation Models》| arxiv2023| 大模型使用工具的综述文章：<br/>1 提出Tool Learning；<br/>2 主要聚焦在Tool-augmented Learning和Tool-oriented Learning两个方面；<br/>3 对比了text-davinci-003和ChatGPT工具使用的情况；| NULL | NULL |
| 《Toolformer: Language Models Can Teach Themselves to Use Tools》| arxiv2023| Meta的ToolFormer：<br/>1 模型自动选择API和填充API的输入，结合API结果获得最终的答案；<br/>2 主要完成类似完形填空的任务，对真实场景的任务感觉借鉴意义不大；<br/>3 结合语言模型的能力自动构建训练集，并实现模型的FineTuning（设计比较巧妙）；<br/>4 主要基于GPT-J进行试验（124M、355M、775M、1.6B），同时，对比了OPT-66B和GPT-3-175B的结果；| NULL | NULL |
| 《LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities》| arxiv2023| 大模型在图谱领域的探索：<br/>1 对比评测大模型对图谱中基础任务的表现（理解类能力不如经典方法，推理能力更强。但文中也说了理解能力的评测不严谨）；<br/>2 评估大模型能力的来源，来自记忆，还是来自真正的理解能力？专门构造了一个大模型之前没见过的虚拟数据集，结论是大模型有真正的理解能力；<br/>3 提出了AutoKG框架，能够自动构建KG和进行推理（整体很模糊）；<br/>4 总体上感觉这篇文章太水了，没什么真正有价值的结论；| NULL | https://mp.weixin.qq.com/s/7DQfUUjCrMRMiPv13CYCpA |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
