|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases》| arxiv2023| 中科院软件所的ToolAlpaca：<br/>1 对API做了统一的格式描述；<br/>2 通过ChatGPT扮演三个角色（userAgent、assistantAgent、toolExecutor）来构建训练数据集；<br/>3 基于Alpaca-7B和Alpaca-13B实现；<br/>4 能力覆盖50个大类，400个API；| NULL | NULL |
| 《TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs》| arxiv2023| 微软的大模型扩展能力框架TaskMatrix：<br/>1 核心组件：Multimodal Conversational Foundation Model (MCFM)、API Platform、API Selector、Action Executor；<br/>2 API选择部分没特别介绍，但对API分组是值得借鉴的；<br/>3 整体设计和ToolAlpaca类似，最核心的就一个模块（原始的上下文、候选API作为输入，直接输出接口编排[参数填充+调用顺序]）；| NULL | NULL |
| 《Gorilla: Large Language Model Connected with Massive APIs》| arxiv2023| 能够调用外部工具API的模型Gorilla：<br/>1 基于LLaMA微调实现；<br/>2 基于self-instruct生成训练指令；<br/>3 基于检索使用候选API（候选API噪声过大，会影响结果）；<br/>4 基于AST做测试集验证；<br/>5 对API限制场景做了研究（很少有这方面的研究）；| NULL | NULL |
| 《OpenAGI: When LLM Meets Domain Experts》| arxiv2023| 罗格斯大学的OpenAGI平台：<br/>1 开源了一个致力于大模型使用工具研究的平台；<br/>2 使用CLIP、BERT、ViT自动打分来评估结果；<br/>3 微调了Vicuna-7B和Flan-T5-Large（770M），小参数的T5效果反而更好（怀疑是训练数据少，同时，RLTF在小模型上更容易训练）；<br/>4 引入了RLTF（a Reinforcement Learning from Task Feedback），过程对结果有影响（刚好利用了平台的自动打分能力）；<br/>5 在生成过程中做了限制（缓解错误生成）；<br/>6 主要偏研究，工具是直接提供的，而且，只生成工具的编排，并没有参数的填充；| NULL | NULL |
| 《Reflexion: Language Agents with Verbal Reinforcement Learning》| arxiv2023| Reflexion：通过反思，提升语言模型执行任务的能力：<br/>1 核心模型：<br/>-Actor（LM）：产生行动<br/>-Evaluator（LM）：对行动进行评价<br/>-SelfReflection（LM）：基于Evaluator的评价，给出对Actor的指导建议<br/>2 整个设计中并不包括经典的强化学习，只是类别了Actor和Evaluator；<br/>3 用了三个LM，主要能够更好的应对失败处理。但SelfReflection并不是对每一步的执行实时反馈，需要完成一个完整的Trial后发现失败，然后，再进行SelfReflection，Actor重新规划并执行新的Trial；| NULL | NULL |
| 《LLM Powered Autonomous Agents》| Blog2023| OpenAI研究员Lilian Weng针对LLM Agents的综述：<br/>1 主要针对三个核心方面展开：<br/>-Planning；<br/>-Memory；<br/>-Tool Use； | NULL | https://lilianweng.github.io/posts/2023-06-23-agent/ |
| 《MRKL Systems：A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning》| arxiv2022| AI21使用LLM提升扩展能力的设计：MRKL<br/>1 有效信息不多，主要是将问题分发到其他的处理单元，在分发时会对传参做格式化处理；<br/>2 基于Jurassic-X实现，主要聚焦在简单数学计算的分析上；| NULL | NULL |
| 《TALM: Tool Augmented Language Models》| arxiv2022| 调用外部工具，辅助回答的方案TALM（机构不明，Google？）：<br/>1 基本过程：<br/>-生成使用工具和参数（工具是text-text的）；<br/>-通过BM25检索工具，并使用工具；<br/>-结合工具执行结果，给出最终答案；<br/>2 基于T5（base、large、XL，参数从220M~3B）finetune实现；<br/>3 Self-Play的原理没看懂（如何计算Loss？和强化学习是如何产生联系的？）| NULL | NULL |
|《ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models》| arxiv2023| ReWOO：一种大模型使用工具的优化框架<br/>1 对标ReAct框架，先整体规划执行步骤，然后，再分别调用外部接口，通过调用LLM两次就能获得最终结果；<br/>2 基于LLaMa-7B进行微调，在部分场景效果超过GPT-3.5<br/>3 随着工具变多，效果下降明显；<br/>4 整体的最终结果一般，最好的ACC也只有70%； | NULL | https://mp.weixin.qq.com/s/8cEBOwUyG0zGlC74IuFNeg |
| 《Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models》| arxiv2022| 清华的Open Delta：<br/>1 关于Delta tuning的综述（大多数预训练参数不变，进行少量参数优化）；<br/>2 解决大模型低成本适配下游任务问题；<br/>3 讲方法总结为三类，Addition-based Methods、Specification-based Methods、Reparameterization-based Methods ；| NULL | GitHub - thunlp/OpenDelta: A plug-and-play library for parameter-efficient-tuning (Delta Tuning) |
| 《OpenPrompt: An Open-source Framework for Prompt-learning》 | arxiv2021 | 清华的OpenPrompt：<br/>1 Prompt Learning的工具包；<br/>2 和OpenDelta是一个团队（OpenPrompt更早）。OpenPrompt主要聚焦在prompt上，而OpenDelta主要聚焦在Delta-Tuning（大模型适配层或者中间层优化）； | NULL | https://zhuanlan.zhihu.com/p/607206925 |
| 《Augmented Language Models: a Survey》 | arxiv2023 | ALM：<br/>1 Yann LeCun参与的关于“增强语言模型”的综述;<br/>2 主要聚焦在Reason、Tools、Act；; | NULL | https://mp.weixin.qq.com/s/oCs4R-xYGS42iXIvgnDCCg |
| 《Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback》| arxiv2023| 微软的大模型增强框架：<br/>1 核心组成部分：<br/>-working memory：对话状态记录；<br/>-policy：执行动作生成，通过规则或则模型实现（文中用了基于强化学习的T5）；<br/>-utility：打分或者反馈，通过规则或者模型实现（文中没有特别声明这部分具体的实现）；<br/>-action executor：执行器，调用外部接口；<br/>2 整体设计感觉比较理想，特别是如果是模型实现policy和utility，不管是性能还是效果，感觉都难以保障；| NULL | NULL |
| 《LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS》| arxiv2022|APE方法：使用大模型来生成prompt<br/>1 在实践中可能速度比较慢，成本比较高（要多次调用大模型才能实现）|NULL |NULL |
| 《Learning by Distilling Context》| arxiv2022 | 上下文蒸馏：<br/>1 给大模型更多的输入提示，同时，要求大模型的输出理由和结果；<br/>2 给小模型输入较少的提示，直接输出最终结果；<br/>3 感觉核心就是通过提示让大模型输出的结果更置信，效果肯定没有纯人工构造的数据好（但机器的效率高）；| NULL | NULL |
| 《HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face》| arXiv2023| 基于ChatGPT和HuggingFace模型接口实现多模型中控（基于gpt-3.5-turbo 和 text-davinci-003模型）：<br/>1 核心目标，将用户原始query通过LLM转变成执行的任务，并最终汇总任务结果，产生回复；<br/>2 核心步骤，Task Planning、Model Selection、Task Execution、Response Generation；<br/>3 整体感觉和之前微软用语言模型控制机器人的设计思路是基本一致的；<br/>4 感觉过于理想，应用在工业场景不一定可靠（Limitations部分提到了不稳定，但感觉表述的还是有些轻描淡写了）； | NULL | NULL |
| 《REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS》| ICLR2023| 谷歌的ReAct：使用大模型作为控制中心（主要是基于3~6个few-shot的示例prompt引导后续的模型行为）：<br/>1 主要基于PaLM-540B进行的实验（也部分对比了GPT-3，text-davinci-002）；<br/>2 虽然摘要里提到相比模拟和强化学习的方案有34%和10%的提升，但最的结果也挺一般的；<br/>3 做了基于PaLM-8/62B的Finetuning，这个效果整体感觉还不错；| NULL | NULL |
| 《WebGPT: Browser-assisted question-answering with human feedback》| arxiv2022| OpenAI的WebGPT：<br/>1 基于GPT-3，借助搜索工具，提升模型的问答能力；<br/>2 核心方法：Behavior cloning（BC）、Reward modeling（RM）、Reinforcement learning（RL）、Reject sampling（best-of-n）；<br/>3 生成结果的引用是直接生成的，没有特别的处理(比如xxxx[1]，其中xxx来自文章1)| NULL | NULL |
| 《Tool Learning with Foundation Models》| arxiv2023| 大模型使用工具的综述文章：<br/>1 提出Tool Learning；<br/>2 主要聚焦在Tool-augmented Learning和Tool-oriented Learning两个方面；<br/>3 对比了text-davinci-003和ChatGPT工具使用的情况；| NULL | NULL |
| 《Toolformer: Language Models Can Teach Themselves to Use Tools》| arxiv2023| Meta的ToolFormer：<br/>1 模型自动选择API和填充API的输入，结合API结果获得最终的答案；<br/>2 主要完成类似完形填空的任务，对真实场景的任务感觉借鉴意义不大；<br/>3 结合语言模型的能力自动构建训练集，并实现模型的FineTuning（设计比较巧妙）；<br/>4 主要基于GPT-J进行试验（124M、355M、775M、1.6B），同时，对比了OPT-66B和GPT-3-175B的结果；| NULL | NULL |
| 《Unlimiformer: Long-Range Transformers with Unlimited Length Input》 | arxiv2023| 解决长文本输入的Unlimiformer：<br/>1 主要应用于Encoder-Decoder场景；<br/>2 先从长文本中检索相关信息，然后，再进行生成生成，过程是动态的；<br/>3 在测试集效果提升上不明显，主要原因是传统的评估手段不能很好的体现出优势（用Entity mentions等方法就比较明显了）； | NULL | NULL |
| 《LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities》| arxiv2023| 大模型在图谱领域的探索：<br/>1 对比评测大模型对图谱中基础任务的表现（理解类能力不如经典方法，推理能力更强。但文中也说了理解能力的评测不严谨）；<br/>2 评估大模型能力的来源，来自记忆，还是来自真正的理解能力？专门构造了一个大模型之前没见过的虚拟数据集，结论是大模型有真正的理解能力；<br/>3 提出了AutoKG框架，能够自动构建KG和进行推理（整体很模糊）；<br/>4 总体上感觉这篇文章太水了，没什么真正有价值的结论；| NULL | https://mp.weixin.qq.com/s/7DQfUUjCrMRMiPv13CYCpA |
| 具身智能  | NULL |NULL |NULL |NULL |
| 《ChatGPT for Robotics:Design Principles and Model Abilities》| 微软研究院2023| 通过ChatGPT进行机器人控制：<br/>1 借助ChatGPT，通过人机对话的方式生成控制机器人的代码；<br/>2 目前还缺少实时的反馈机制（论文的结论和未来规划中也提到了，后续的工作可以进行尝试）；| NULL | https://mp.weixin.qq.com/s/ahWFcsq9lurPbKi0-8705g |
| 《Do As I Can, Not As I Say:Grounding Language in Robotic Affordances》| arxiv2022| Google的SayCan：使用语言模型控制机器人<br/>1 预设一些操作指令（7个families，101个instructions）；<br/>2 基于PaLM-540B作为LLM；<br/>3 接收用户的指令后，LLM通过Decoder预测所有操作指令（101个）的概率值P-llm；<br/>4 价值函数计算出所有操作指令（101个）的价值Q-pi(s,a)，其中价值函数基于RL和BC两种方式实现（附录中有实现细节），通过最终episode完成得分1，否则得分0，来进行模型训练；<br/>5 最终取P-llmxQ-pi(s,a)的最大值指令，进行执行；<br/>6 LLM和价值函数部分因为要遍历所有的操作指令进行打分，整体的执行效率比较低； | NULL | NULL |
| 《VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models》 | arxiv2023 | 李飞飞的具身智能VoxPoser：<br/>1 基于GPT-4实现机器手臂控制；<br/>2 先讲用户的指令拆解成子任务，然后，在基于子任务进行执行（整个框架是预设好的，调用的API也是确定的几个）；<br/>3 整个过程中没有实时反馈；<br/>4 子任务执行过程中设计到机器臂控制的细节（比如，先目标检测，再语义分割，以及先识别出感兴趣的目标和需要规避的目标），这些细节导致整个paper理解起来有些困难（需要机器人的背景？）； | NULL | NULL |
| 《PaLM-E: An Embodied Multimodal Language Model》| arxiv2023| Goolge的具身语言模型PaLM-E：<br/>1 模型实现：PaLM（540B，Decoder）+ViT（22B）<br/>2 支持文本、图像等多种模态信息（输入信息感觉有些复杂）<br/>-State estimation vectors<br/>-Vision Transformer (ViT)<br/>-Object-centric representations<br/>-Object Scene Representation Transformer (OSRT)<br/>-Entity referrals<br/>3 主要聚焦在多模的输入、融合训练上和多场景评测上，并没有特别介绍模型是如何对机器人进行控制的（主要参考SayCan的工作）；| NULL | NULL |
| 《RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation》| arxiv2023| DeepMind的具身智能大模型RoboCat：<br/>1 和Google的PaLM-E一样，主要聚焦在大模型本身，并没有具体介绍任务编排相关的架构设计（具体的细节是怎样的？有哪些API？具体的实现是对齐之前的某项工作？）；<br/>2 核心思想是：<br/>-先有一个通用的基础模型；<br/>-基于通用基础模型finetune训练一个特殊场景的模型；<br/>-使用特殊场景的模型自动生成数据；<br/>-将特殊场景模型生成的数据，和之前的所有数据放在一起，重新训练通用基础模型，形成迭代；<br/>3 主干模型基于Gato，视觉编码器基于VQ-CAN（参数是冻住的，但针对现在的控制场景做了预训练）；| NULL | NULL |
| 《RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE》| arxiv2022| Google的具身智能机器人RT-1：SayCan和Gato之后的工作<br/>1 机器人平台：Everyday Robots<br/>2 模型实现：<br/>-通过EfficientNet-B3完成图片的嵌入（连续6张图片）；<br/>-通过Sentence Encoder完成用户输入文本指令的嵌入；<br/>-通过FiLM完成图片+指令文本的嵌入；<br/>-通过TokenLearner将图片+文本融合后的嵌入压缩到只有8个token；<br/>-通过Transfer输出最终的结果；<br/>-最终结果由mode、arm、base三部分组成（不是API，而是基本的运动参数）；<br/>3 整个机器的最终控制，并不是由API编排实现，而是直接操控最基本的机器人控制元素（比如，movement (x, y, z, roll, pitch, yaw, opening of the gripper)）；<br/>4 没太能力理解，机器人是如何感知空间信息的，是因为“记忆”了空间信息？这样换个房间应该就不行了（训练的房间和测试的房间不一致）？| NULL | NULL |
| 《TOWARDS A UNIFIED AGENT WITH FOUNDATION MODELS》| ICLR2023| DeepMind提出的用于训练多模态智能体的范式：<br/>1 核心并不是介绍智能体是如何构建的，主要聚焦在训练数据的构建；<br/>-在模拟环境中进行实验：MuJoCo physics simulator <br/>-通过语言模型（FLAN-T5）把任务拆解成多个步骤；<br/>-通过CLIP做步骤文本和场景图片的匹配，生成过程中的奖励，从而解决稀疏奖励的问题；<br/>2 创新点总结：<br/>-有效地探索稀疏奖励环境；<br/>-重新使用收集到的数据启动新任务的顺序学习；<br/>-安排学习技能来解决新任务；| NULL | https://mp.weixin.qq.com/s/WbGSo0Xys4Zy17yrkhZEvg |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
