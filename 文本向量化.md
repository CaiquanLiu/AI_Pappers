|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《Efficient Estimation of Word Representations in Vector Space》| NULL|Tomas Mikolov的word2vec论文：<br/>1 介绍了CBOW和skip-gram方法；<br/>2 论文比较难懂；|NULL |https://zhuanlan.zhihu.com/p/39751353|
| 《Distributed Representations of Words and Phrases and their Compositionality》| NULL |Tomas Mikolov的word2vec论文：<br/>1 介绍了Hierarchical Softmax和Negative Sampling方法；<br/>2 论文比较难懂；|NULL |NULL |
| 《word2vec Parameter Learning Explained》| arXiv2016|Tomas Mikolov的word2vec论文解读：<br/>1 理论完备由浅入深非常好懂，且直击要害，既有 high-level 的 intuition 的解释，也有细节的推导过程。一定要看这篇paper！一定要看这篇paper！一定要看这篇paper！|NULL |https://zhuanlan.zhihu.com/p/53425736|
| 《word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method》| arXiv2014|Tomas Mikolov的word2vec论文解读：<br/>1 对negative-sampling做了详细的解释|NULL |NULL |
| FastText  | NULL |注意输入部分：字符嵌入？词根嵌入？ |NULL |https://research.fb.com/downloads/fasttext/ |
| 《A Neural Probabilistic Language Model》  | JMLR2003 |Benjo的神经语言模型 |NULL |NULL |
| 《Universal Sentence Encoder for English》  | NULL |NULL |NULL |NULL |
| 《Distributed Representations of Sentences and Documents》| ICML2014|段向量：<br/>1 同word2vec一样存在CBOW和Skip-gram两种方案；<br/>2 段落向量要离线训练好，不可能实时训练获得；|done|NULL |
| 《A Convolutional Neural Network for Modelling Sentences》| ACL2014|基于多层CNN的句子表示：<br/>1 不像经典的CNN，对字/词向量的整个维度进行卷积，而是，对每一维进行卷积。这样保证，卷积操作后，还是保持矩阵形式，进而，能进行多层卷积操作；<br/>2 处理卷积操作特殊外，也还引入了多种特殊的处理手段（单维度卷积、动态池化、折叠操作等）；|done|https://zhuanlan.zhihu.com/p/29925124|
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
