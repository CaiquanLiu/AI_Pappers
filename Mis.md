|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering》  | COLING 2018 |NULL |NULL |NULL|
| 《Analysis Methods in Neural Language Processing: A Survey》  | 2019 |NULL |NULL |NULL |
| 《Deep Double Descent: Where Bigger Models and More Data Hurt》  | NULL |NULL |NULL |NULL |
| 《中文信息处理发展报告》  | 2016 |NULL |NULL |NULL |
| 《Modern Deep Learning Techniques Applied to Natural Language Processing》  | 博文 |NLP综述 |NULL |https://nlpoverview.com/index.html |
| 《Counterfactual Off-Policy Training for Neural Response Generation》| EMNLP2020|其他：生成式对话，反事实推理、GAN；|NULL |https://mp.weixin.qq.com/s/__edXubTKVZnl5mCU9lWkg|
| 《Is Graph Structure Necessary for Multi-hop Question Answering?》| EMNLP2020|其他：图结构对于多步推理问答任务是否是必要的？（非必要）|NULL |https://mp.weixin.qq.com/s/mXLrcg0ZSaKF4w9pqv5_8w|
| 《Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach》| arXiv2020|其他：作者则提出了一个新的对比学习loss SCL，将同一类的样本互相作为正例，不同类别的作为负例。以此达到拉近类内样本、拉开类间距离的目的。|NULL |https://mp.weixin.qq.com/s/7AgKlzu3-o1UpsnvwR7m4g|
| 《RETHINKING ATTENTION WITH PERFORMERS》| arXiv2020|其他：Performer 使用一个高效的（线性）广义注意力框架（generalized attention framework），允许基于不同相似性度量（核）的一类广泛的注意力机制|NULL |https://mp.weixin.qq.com/s/8vk79ppv6JGDAdgWL_BQRw|
| 《Adversarial Training for Large Neural LangUage Models》| arXiv2020|其他：本文把对抗训练用到了预训练和微调两个阶段|NULL |https://mp.weixin.qq.com/s/UDZjUZUnuD7buMb-CWIzuA|
| 《VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training》| arXiv2020|其他：VIVO可以在没有文本标签的数据上进行文本和图像的多模态预训练|NULL |https://mp.weixin.qq.com/s/Cl0NyjnifwE454vrhq7Mww|
| 《Cognitive Graph for Multi-Hop Reading Comprehension at Scale》| ACL2019|其他：认知系统system1+system2|NULL |https://zhuanlan.zhihu.com/p/260929857|
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
