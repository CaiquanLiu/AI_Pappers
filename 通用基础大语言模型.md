|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《PEER: A Collaborative Language Model》| arxiv2022| Meta的PEEK：<br/>1 交互式文本编辑语言模型；<br/>2 训练数据来源维基百科的更新历史和自己的数据增强；<br/>3 整体上和InstructGPT（text-davinc-001）相比，在个别场景优势不明显；| NULL | NULL |
| 《Training language models to follow instructions with human feedback》 | arxiv 2022 | OpenAI的InstructGPT：<br/>训练了三个模型（1.3B、6B、175B），其中1.3B的效果就比175B的GPT-3效果好；<br/>主要的核心SFT+MR+PPO；<br/>训练的数据集也比较小：SFT-13k，RM-33k，PPO-31k | NULL | NULL |
| 《Scaling Instruction-Finetuned Language Models》| arXiv2022| Google的FLAN：<br/> Instruct tuning；<br/>基于PaLM和T5进行的实验；| NULL | NULL |
| 《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》| arXiv2020| 英伟达的Megatron LM：<br/>核心是张量并行（数据并行、张量并行、Pipeline并行）；<br/>在BERT和GPT-2上做了验证；<br/>只需做部分代码插入，不需要专门的框架或者编译器；<br/>李沐的paper分享也有介绍；| NULL |参考：https://zhuanlan.zhihu.com/p/366906920|
| 《LaMDA: Language Models for Dialog Applications》| arxiv2022| 谷歌大脑的LaMDA语言模型：<br/>1 训练范式相对简单，pretrain+finetune（对比InstructGPT的SFT+RM+RLHF）；<br/>2 引入了检索结果；<br/>3 对回复的评价测量，Quality，Safty，Groundedness（对应InstructGPT的helpful，harmfulless，honest）；| NULL | NULL |
| 《Improving alignment of dialogue agents via targeted human judgements》| arxiv2022| DeepMind的Sparrow语言模型：<br/>1 融合了Lambda模型的外部知识，和InstructGPT的RLHF；<br/>2 基础模型基于Chinchilla；| NULL | NULL |
| 《LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS》| arxiv2022|APE方法：使用大模型来生成prompt<br/>1 在实践中可能速度比较慢，成本比较高（要多次调用大模型才能实现）|NULL |NULL |
| 《LLaMA: Open and Efficient Foundation Language Models》| facebook research 2023| Meta的LLaMA模型：<br/>1 Meta的高效率大模型（参数更小）；<br/>2 LLaMA-13B对标GPT-3(175B)；<br/>3 LLaMA-65B对标Chinchilla70B and PaLM-540B；<br/>4 核心是数据量的和计算量的提升（原理和Deepmind的Chinchilla基本一致）；| NULL | https://mp.weixin.qq.com/s/MbZTfVgxx221Eo9pl1h80w |
| 《Learning by Distilling Context》| arxiv2022 | 上下文蒸馏：<br/>1 给大模型更多的输入提示，同时，要求大模型的输出理由和结果；<br/>2 给小模型输入较少的提示，直接输出最终结果；<br/>3 感觉核心就是通过提示让大模型输出的结果更置信，效果肯定没有纯人工构造的数据好（但机器的效率高）；| NULL | NULL |
| 《Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback》 | arxiv2022 | Anthropic对话大模型实现：<br/>1 主要侧重了Helpful和Harmfulless，Honest较少（比如单纯的Helpful vs Helpful+Harmfulless的对比）；<br/>2 有red-team专门针对Harmful开展工作；<br/>3 Align tax随着模型规模变大，变的不明显；<br/>4 整体看起来比较累，太多的实验细节（对实际工作也更有帮助）； | NULL | https://zhuanlan.zhihu.com/p/605336680 |
| 《Constitutional AI: Harmlessness from AI Feedback》| arxiv2022| Anthropic公司的模型（创始团队来自OpenAI，新产品Claude）：<br/>1 整体设计思路和InstructGPT差不多（SFT、RM、RLHF）；<br/>2 SFT阶段引入了Constitutional AI机制，核心是对harmless的内容的处理（harmless、honest、helpful），基于Critique/Revision；<br/>3 使用PM替换了RM（PM使用的数据部分是Constitutional AI自动生成的）； | NULL | 参考：https://mp.weixin.qq.com/s/si2M52H1qKnyHd0IHVJR8g|
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
