|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| OpenAI  | OpenAI |OpenAI |OpenAI |OpenAI |
| 《Let’s Verify Step by Step》| arxiv2023| OpenAI的过程监督方法：<br/>1 过程监督可以训练出比结果监督更可靠的奖励模型；<br/>2 对于较小的reward model，大reward model可以可靠地近似于人类监督，并且可以用于有效地进行大规模数据收集消融；<br/>3 主动学习可以使过程监督的数据效率提高2.6倍；<br/>4 发布了全过程监督数据集PRM800K，以促进相关研究| NULL | 蒟蒻：Let's Verify Step by Step |
| 《Improving Language Understanding by Generative Pre-Training》  | 2018，没发表？ |OpenAI GPT：<br/> 先于BERT提出Pre-train+Fine-tuning范式; |done |NULL |
| 《Language Models are Unsupervised Multitask Learners》  | 2019，没发表？ |GPT-2：<br/>数据集WebText；<br/>Train一个参数超大的语言模型，实现Zero-shot效果；<br/>没有Fine-tuning过程； |done |NULL |
| 《Language Models are Few-Shot Learners》  | NULL |GPT-3：<br/>实现few-shot和one-shot，不需要梯度更新；<br/>可以Fine-tuning，但paper中，没用Fine-tuning；<br/>one-shot和few-shot体现在输入中，而不会用来微调，可以参考paper中的Figure 2.1； |done |NULL |
| 《Training language models to follow instructions with human feedback》 | arxiv 2022 | OpenAI的InstructGPT：<br/>训练了三个模型（1.3B、6B、175B），其中1.3B的效果就比175B的GPT-3效果好；<br/>主要的核心SFT+MR+PPO；<br/>训练的数据集也比较小：SFT-13k，RM-33k，PPO-31k | NULL | NULL |
| 《GPT-4 Technical Report》| arxiv2023| 《GPT-4 Technical Report》<br/>1 Introduction<br/>2 Scope and Limitations of this Technical Report<br/>3 Predictable Scaling<br/>4 Capabilities<br/>5 Limitations<br/>6 Risks & mitigations<br/>7 Conclusion<br/>Authorship, Credit Attribution, and Acknowledgements<br/><br/>《GPT-4 System Card》<br/>1 Introduction<br/>2 GPT-4 Observed Safety Challenges<br/>3 Deployment Preparation<br/>4 System Safety<br/>5 Conclusion and Next Steps<br/>6 Acknowledgements| NULL | NULL |
| Google  | Google |Google |Google |Google |
| 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》  | NULL |Google T5：<br>模型结构：Encoder-Decoder、Language model和Prefix LM;<br>注意力掩码机制：Casual、Casual prefix和Fully-visible;<br>预训练方法：语言模型式、Bert Mask式和Deshuffling;<br>Mask策略：Mask模式、Replace span模式和drop模式;<br>Mask比例：10%、15%、25%和50%四种;<br>Span长度：2、3、5和10四种;<br>训练策略：fine-tune、多任务学习、多任务fine-tune和Scaling; |done |博客参考：<br/> johnchenyhl：Google预训练语言模型T5 <br/> 机器之心：谷歌T5预训练模型单次运行成本超130万美元？算力和金钱才是模型训练的王道
| 《Scaling Instruction-Finetuned Language Models》| arXiv2022| Google的FLAN：<br/> Instruct tuning；<br/>基于PaLM和T5进行的实验；| NULL | NULL |
| 《LaMDA: Language Models for Dialog Applications》| arxiv2022| 谷歌大脑的LaMDA语言模型：<br/>1 训练范式相对简单，pretrain+finetune（对比InstructGPT的SFT+RM+RLHF）；<br/>2 引入了检索结果；<br/>3 对回复的评价测量，Quality，Safty，Groundedness（对应InstructGPT的helpful，harmfulless，honest）；| NULL | NULL |
| 《PaLM 2 Technical Report》| arxiv2023| Google大模型PaLM2技术报告：<br/>1 Introduction<br/>2 Scaling law experiments<br/>3 Training dataset<br/>4 Evaluation<br/>5 Responsible usage| NULL | NULL |
| 《Deep Reinforcement Learning from Human Preferences》| arXiv2017| 基于人类反馈的强化学习：<br/>1 ChatGPT中使用的RLHF的起源方法；<br/>2 OpenAI和DeepMind的联合工作；<br/>3 在PPO方法出现之前（文中使用A2C和TRPO）；| NULL | NULL |
| DeepMind  | DeepMind |DeepMind |DeepMind |DeepMind |
| 《Solving math word problems with processand outcome-based feedback》| arxiv2022| DeepMind优化大模型解决数学问题的能力：<br/>1 Outcome-based and process-based approaches lead to similar final-answer error rates（这个结论和OpenAI的结论是不一致的）；<br/>2 Both process- and outcome-supervised reward models learn to emulate process-based feedback；<br/>3 Low trace error requires either process-based feedback, or a reward model that emulates it；| NULL | NULL |
| 《Improving alignment of dialogue agents via targeted human judgements》| arxiv2022| DeepMind的Sparrow语言模型：<br/>1 融合了Lambda模型的外部知识，和InstructGPT的RLHF；<br/>2 基础模型基于Chinchilla；| NULL | NULL |
| Meta  | Meta |Meta |Meta |Meta |
| 《LLaMA: Open and Efficient Foundation Language Models》| facebook research 2023| Meta的LLaMA模型：<br/>1 Meta的高效率大模型（参数更小）；<br/>2 LLaMA-13B对标GPT-3(175B)；<br/>3 LLaMA-65B对标Chinchilla70B and PaLM-540B；<br/>4 核心是数据量的和计算量的提升（原理和Deepmind的Chinchilla基本一致）；| NULL | Meta发布全新大语言模型，号称比ChatGPT更强，单GPU上就能跑，后续或将开源 |
| 《PEER: A Collaborative Language Model》| arxiv2022| Meta的PEEK：<br/>1 交互式文本编辑语言模型；<br/>2 训练数据来源维基百科的更新历史和自己的数据增强；<br/>3 整体上和InstructGPT（text-davinc-001）相比，在个别场景优势不明显；| NULL | NULL |
| Anthropic  | Anthropic |Anthropic |Anthropic |Anthropic |
| 《Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback》 | arxiv2022 | Anthropic对话大模型实现：<br/>1 主要侧重了Helpful和Harmfulless，Honest较少（比如单纯的Helpful vs Helpful+Harmfulless的对比）；<br/>2 有red-team专门针对Harmful开展工作；<br/>3 Align tax随着模型规模变大，变的不明显；<br/>4 整体看起来比较累，太多的实验细节（对实际工作也更有帮助）； | NULL | 苏打：ChatGPT技术精要，RLHF相关论文笔记（三）—— a Helpful and Harmless Assistant with Reinforcement Learning from Human |
| 《Constitutional AI: Harmlessness from AI Feedback》| arxiv2022| Anthropic公司的模型（创始团队来自OpenAI，新产品Claude）：<br/>1 整体设计思路和InstructGPT差不多（SFT、RM、RLHF）；<br/>2 SFT阶段引入了Constitutional AI机制，核心是对harmless的内容的处理（harmless、honest、helpful），基于Critique/Revision；<br/>3 使用PM替换了RM（PM使用的数据部分是Constitutional AI自动生成的）； | NULL | 参考：网页链接
| 其他  | NULL |NULL |NULL |NULL |
| 《OpenAssistant Conversations - Democratizing Large Language Model Alignment》 | arxiv2023 | 非盈利组织LAION公司的开源大模型：<br/>1 LAION公司为Stable Diffusion公司提供了数据；<br/>2 文章的重点在对数据的介绍，模型相关的细节不多；<br/>3 SFT模型：Pythia-12B、LLaMA-13B、LLaMA-30B；<br/>4 PM模型：Pythia-1.4B、Pythia-12B；LLaMA-30B（计划）；<br/>5 PPO训练框架基于TRLX； | NULL | NULL|
| 《RRHF: Rank Responses to Align Language Models with Human Feedback without tears》| arXiv2023| 阿里达摩院的RRHF：<br/>1 不使用强化学习实现RLHF；<br/>2 传统的方法使用4个模型：LanguageModel、ValueModel、RewardModel、ReferenceModel，RRHF只使用2个：LanguageModel、RewardModel；<br/>3 核心思想是使用LanguageModel对其他模型（或者策略）生成的结果打分，和ReferenceModel的打分尽量一致；<br/>4 从实现效果看RRHF效果略好过PPO的方式；| NULL | NULL |
| 《A Survey of Large Language Models》 | arXiv2023 | 大模型综述：作者主要来自人民大学高瓴人工智能学院（v9,20230428）<br/>1 INTRODUCTION<br/>2 OVERVIEW<br/>2.1 Background for LLMs<br/>2.2 Technical Evolution of GPT-series Models<br/>3 RESOURCES OF LLMS<br/>3.1 Publicly Available Model Checkpoints or APIs<br/>3.2 Commonly Used Corpora<br/>3.3 Library Resource<br/>4 PRE-TRAINING<br/>4.1 Data Collection<br/>4.2 Architecture<br/>4.3 Model Training<br/>5 ADAPTATION TUNING OF LLMS<br/>5.1 Instruction Tuning<br/>5.2 Alignment Tuning<br/>5.3 Efficient Tuning<br/>6 UTILIZATION<br/>6.1 In-Context Learning<br/>6.2 Chain-of-Thought Prompting<br/>7 CAPACITY EVALUATION<br/>7.1 Basic Evaluation Tasks<br/>7.2 Advanced Ability Evaluation<br/>7.3 Public Benchmarks and Empirical Analysis<br/>8 CONCLUSION AND FUTURE DIRECTIONS | NULL | LLM+人工校对中文版：https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese_0418.pdf |
| 《Direct Preference Optimization:Your Language Model is Secretly a Reward Model》 | arxiv2023 | NULL | DPO:<br/>1 DPO只使用了2个模型（Policy Model+Reference Model），RLHF使用了4个（Policy Model+Reference Model+Reward Model+Critic Model）；<br/>2 训练语料为三元组：用户prompt，良好回答win，一般回答lose；<br/>3 损失函数能够明显的体现出良好回复和一般回复对最终目标的影响；<br/>4 相比与SFT，除了良好的回复反馈外，还有一般的回复反馈；<br/>5 是否有像RLHF一样（强化学习），对过程无监督，保障生成结果多样性的机制？<br/>6 设计较多原理推导，不怎么看得懂； | mengrennwpu：LLM面面观之RLHF平替算法DPO |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
