|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| NULL  | NULL |NULL |NULL |NULL |
| 《ChatGPT for Robotics:Design Principles and Model Abilities》| 微软研究院2023| 通过ChatGPT进行机器人控制：<br/>1 借助ChatGPT，通过人机对话的方式生成控制机器人的代码；<br/>2 目前还缺少实时的反馈机制（论文的结论和未来规划中也提到了，后续的工作可以进行尝试）；| NULL | https://mp.weixin.qq.com/s/ahWFcsq9lurPbKi0-8705g |
| 《Do As I Can, Not As I Say:Grounding Language in Robotic Affordances》| arxiv2022| Google的SayCan：使用语言模型控制机器人<br/>1 预设一些操作指令（7个families，101个instructions）；<br/>2 基于PaLM-540B作为LLM；<br/>3 接收用户的指令后，LLM通过Decoder预测所有操作指令（101个）的概率值P-llm；<br/>4 价值函数计算出所有操作指令（101个）的价值Q-pi(s,a)，其中价值函数基于RL和BC两种方式实现（附录中有实现细节），通过最终episode完成得分1，否则得分0，来进行模型训练；<br/>5 最终取P-llmxQ-pi(s,a)的最大值指令，进行执行；<br/>6 LLM和价值函数部分因为要遍历所有的操作指令进行打分，整体的执行效率比较低； | NULL | NULL |
| 《VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models》 | arxiv2023 | 李飞飞的具身智能VoxPoser：<br/>1 基于GPT-4实现机器手臂控制；<br/>2 先讲用户的指令拆解成子任务，然后，在基于子任务进行执行（整个框架是预设好的，调用的API也是确定的几个）；<br/>3 整个过程中没有实时反馈；<br/>4 子任务执行过程中设计到机器臂控制的细节（比如，先目标检测，再语义分割，以及先识别出感兴趣的目标和需要规避的目标），这些细节导致整个paper理解起来有些困难（需要机器人的背景？）； | NULL | NULL |
| 《PaLM-E: An Embodied Multimodal Language Model》| arxiv2023| Goolge的具身语言模型PaLM-E：<br/>1 模型实现：PaLM（540B，Decoder）+ViT（22B）<br/>2 支持文本、图像等多种模态信息（输入信息感觉有些复杂）<br/>-State estimation vectors<br/>-Vision Transformer (ViT)<br/>-Object-centric representations<br/>-Object Scene Representation Transformer (OSRT)<br/>-Entity referrals<br/>3 主要聚焦在多模的输入、融合训练上和多场景评测上，并没有特别介绍模型是如何对机器人进行控制的（主要参考SayCan的工作）；| NULL | NULL |
| 《RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation》| arxiv2023| DeepMind的具身智能大模型RoboCat：<br/>1 和Google的PaLM-E一样，主要聚焦在大模型本身，并没有具体介绍任务编排相关的架构设计（具体的细节是怎样的？有哪些API？具体的实现是对齐之前的某项工作？）；<br/>2 核心思想是：<br/>-先有一个通用的基础模型；<br/>-基于通用基础模型finetune训练一个特殊场景的模型；<br/>-使用特殊场景的模型自动生成数据；<br/>-将特殊场景模型生成的数据，和之前的所有数据放在一起，重新训练通用基础模型，形成迭代；<br/>3 主干模型基于Gato，视觉编码器基于VQ-CAN（参数是冻住的，但针对现在的控制场景做了预训练）；| NULL | NULL |
| 《RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE》| arxiv2022| Google的具身智能机器人RT-1：SayCan和Gato之后的工作<br/>1 机器人平台：Everyday Robots<br/>2 模型实现：<br/>-通过EfficientNet-B3完成图片的嵌入（连续6张图片）；<br/>-通过Sentence Encoder完成用户输入文本指令的嵌入；<br/>-通过FiLM完成图片+指令文本的嵌入；<br/>-通过TokenLearner将图片+文本融合后的嵌入压缩到只有8个token；<br/>-通过Transfer输出最终的结果；<br/>-最终结果由mode、arm、base三部分组成（不是API，而是基本的运动参数）；<br/>3 整个机器的最终控制，并不是由API编排实现，而是直接操控最基本的机器人控制元素（比如，movement (x, y, z, roll, pitch, yaw, opening of the gripper)）；<br/>4 没太能力理解，机器人是如何感知空间信息的，是因为“记忆”了空间信息？这样换个房间应该就不行了（训练的房间和测试的房间不一致）？| NULL | NULL |
| 《TOWARDS A UNIFIED AGENT WITH FOUNDATION MODELS》| ICLR2023| DeepMind提出的用于训练多模态智能体的范式：<br/>1 核心并不是介绍智能体是如何构建的，主要聚焦在训练数据的构建；<br/>-在模拟环境中进行实验：MuJoCo physics simulator <br/>-通过语言模型（FLAN-T5）把任务拆解成多个步骤；<br/>-通过CLIP做步骤文本和场景图片的匹配，生成过程中的奖励，从而解决稀疏奖励的问题；<br/>2 创新点总结：<br/>-有效地探索稀疏奖励环境；<br/>-重新使用收集到的数据启动新任务的顺序学习；<br/>-安排学习技能来解决新任务；| NULL | https://mp.weixin.qq.com/s/WbGSo0Xys4Zy17yrkhZEvg |
