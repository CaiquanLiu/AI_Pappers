|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| NULL  | NULL |NULL |NULL |NULL |
| 《EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION》 | arxiv2023| Meta对RoPE的优化，扩展大模型的输入窗口：<br/>1 应用在LLaMA上，输入窗口从2k扩展到32k；<br/>2 核心是优化了之前RoPE的f（x,m）| NULL | https://zhuanlan.zhihu.com/p/640696998 <br/> https://zhuanlan.zhihu.com/p/640722266 |
| 《ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING》| arxiv2021| 苏剑林提出的RoPE：RoFormer<br/>1 核心是引入了旋转矩阵f(xi)=R*W*xi，其中，R是旋转矩阵，W是QKV变换权重，xi是之前的token嵌入；<br/>2 之前的方案（绝对位置编码，相对位置编码），f(xi)=W(xi+pi)，其中，W、xi同上，pi是位置编码； | NULL | NULL |
| 《Unlimiformer: Long-Range Transformers with Unlimited Length Input》 | arxiv2023| 解决长文本输入的Unlimiformer：<br/>1 主要应用于Encoder-Decoder场景；<br/>2 先从长文本中检索相关信息，然后，再进行生成生成，过程是动态的；<br/>3 在测试集效果提升上不明显，主要原因是传统的评估手段不能很好的体现出优势（用Entity mentions等方法就比较明显了）； | NULL | NULL |
