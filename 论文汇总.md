|编号  |  名称   | 目录  |
|  ----  | ----  |----  | 
| 1  | 《A State-of-the-Art Survey on Deep Learning Theory and Architectures》 |机器学习&深度学习综述 |
| 2  | 《Deep Learning》 |机器学习&深度学习综述 |
| 3  | 《learning internal representation by error propagation》 |DNN |
| 4  | 《LONG SHORT-TERM MEMORY》 |RNN |
| 5  | 《Gradient-Based Learning Applied to Document Recognition》 |CNN |
| 6  | 《Attention Is All You Need》 |Transformer |
| 7  | 《Star-Transformer》 |Transformer |
| 8  | 《Transformer-XL: Language Modeling with Longer-Term Dependency》 |Transformer |
| 9  | 《An Introductory Survey on Attention Mechanisms in NLP Problems》 |Attention机制&指针网络&记忆网络&深度图灵机 |
| 10  | 《Pointer Networks》 |Attention机制&指针网络&记忆网络&深度图灵机 |
| 11  | 《Attention-Based LSTM for Psychological Stress Detection from Spoken Language Using Distant Supervision》 |Attention机制&指针网络&记忆网络&深度图灵机 |
| 12  | 《End-To-End Memory Networks》 |Attention机制&指针网络&记忆网络&深度图灵机 |
| 13  | 《Neural Turing Machines》 |Attention机制&指针网络&记忆网络&深度图灵机 |
| 14  | 《Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey》 |对抗攻击 |
| 15  | 《TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP》 |对抗攻击 |
| 16  | 《Tackling Graphical NLP problems with Graph Recurrent Networks》 |图网络 |
| 17  | 《阿尔伯塔大学博士毕业论文：基于图结构的自然语言处理》 |图网络 |
| 18  | 《Convolutional Networks on Graphs for Learning Molecular Fingerprints》 |图网络 |
| 19  | 《SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS》 |图网络 |
| 20  | 《The graph neural network model》 |图网络 |
| 21  | 《Deepwalk: Online learning of social representations》 |图网络 |
| 22  | 《SkipGram model on the generated random walks》 |图网络 |
| 23  | 《HOW POWERFUL ARE GRAPH NEURAL NETWORKS?》 |图网络 |
| 24  | 《Learning Convolutional Neural Networks for Graphs》 |图网络 |
| 25  | 《Graph Neural Networks:A Review of Methods and Applications》 |图网络 |
| 26  | 《A Comprehensive Survey on Graph Neural Networks》 |图网络 |
| 27  | 《Deep Learning on Graphs: A Survey》 |图网络 |
| 28  | 《LINE: Large-scale Information Network Embedding》 |图网络 |
| 29  | 《Graph Attention Networks》 |图网络 |
| 30  | 《struc2vec: Learning Node Representations from Structural Identity》 |图网络 |
| 31  | 《GraphSAINT: Graph Sampling Based Inductive Learning Method》 |图网络 |
| 32  | 《GraphSage: Graph Sampling Based Inductive Learning Method》 |图网络 |
| 33  | 《DROPEDGE: TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION》 |图网络 |
| 34  | 《Evolution of Transfer Learning in Natural Language Processing》 |迁移学习 |
| 35  | 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》 |迁移学习 |
| 36  | 《Domain-Adversarial Training of Neural Networks》 |领域适应 |
| 37  | 《Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms》 |AutoML |
| 38  | 《AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data》 |AutoML |
| 39  | 《A Survey on Causal Inference》 |因果推断 |
| 40  | 《迈向第三代人工智能》 |常识AI |
| 41  | 《DEEP LEARNING FOR SYSTEM 2 PROCESSING，YOSHUA BENGIO》 |常识AI |
| 42  | 《FROM SYSTEM 1 DEEP LEARNING TO SYSTEM 2 DEEP LEARNING,YOSHUA BENGIO》 |常识AI |
| 43  | 《Cognitive Graph for Multi-Hop Reading Comprehension at Scale》 |常识AI |
| 44  | 《Auto-Encoding Variational Bayes》 |贝叶斯神经网络 |
| 45  | 《Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks》 |神经逻辑推理 |
| 46  | 《Neural Logic Reasoning》 |神经逻辑推理 |
| 47  | 《Recent Trends in Deep Learning Based Natural Language Processing》 |自然语言处理综述 |
| 48  | 《Pre-trained Models for Natural Language Processing: A Survey》 |文本预训练模型 |
| 49  | 《Semi-supervised Sequence Learning》 |文本预训练模型 |
| 50  | 《Learned in Translation: Contextualized Word Vectors》 |文本预训练模型 |
| 51  | 《Semi-supervised sequence tagging with bidirectional language models》 |文本预训练模型 |
| 52  | 《Universal Language Model Fine-tuning for Text Classification》 |文本预训练模型 |
| 53  | 《Deep contextualized word representation》 |文本预训练模型 |
| 54  | 《Improving Language Understanding by Generative Pre-Training》 |文本预训练模型 |
| 55  | 《Language Models are Unsupervised Multitask Learners》 |文本预训练模型 |
| 56  | 《Language Models are Few-Shot Learners》 |文本预训练模型 |
| 57  | 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 |文本预训练模型 |
| 58  | 《Cross-lingual Language Model Pretraining》 |文本预训练模型 |
| 59  | 《MASS: Masked Sequence to Sequence Pre-training for Language Generation》 |文本预训练模型 |
| 60  | 《Unified Language Model Pre-training for Natural Language Understanding and Generation》 |文本预训练模型 |
| 61  | 《UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training》 |文本预训练模型 |
| 62  | 《MPNet: Masked and Permuted Pre-training for Language Understanding》 |文本预训练模型 |
| 63  | 《RoBERTa: A Robustly Optimized BERT Pretraining Approach》 |文本预训练模型 |
| 64  | 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》 |文本预训练模型 |
| 65  | 《Pre-Training with Whole Word Masking for Chinese BERT》 |文本预训练模型 |
| 66  | 《ERNIE: Enhanced Language Representation with Informative Entities》 |文本预训练模型 |
| 67  | 《ERNIE: Enhanced Representation through Knowledge Integration》 |文本预训练模型 |
| 68  | 《ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FORLANGUAGE》 |文本预训练模型 |
| 69  | 《ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators》 |文本预训练模型 |
| 70  | 《K-BERT: Enabling Language Representation with Knowledge Graph》 |文本预训练模型 |
| 71  | 《StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding》 |文本预训练模型 |
| 72  | 《Semantics-aware BERT for Language Understanding》 |文本预训练模型 |
| 73  | 《XLNet: Generalized Autoregressive Pretraining for Language Understanding》 |文本预训练模型 |
| 74  | 《Longformer: The Long-Document Transformer》 |文本预训练模型 |
| 75  | 《Big Bird: Transformers for Longer Sequences》 |文本预训练模型 |
| 76  | 《Well-Read Students Learn Better: On the Importance of Pre-training Compact Models》 |文本预训练模型 |
| 77  | 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》 |文本预训练模型 |
| 78  | 《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》 |文本预训练模型 |
| 79  | 《TinyBERT: Distilling BERT for Natural Language Understanding》 |文本预训练模型 |
| 80  | 《FastBERT: a Self-distilling BERT with Adaptive Inference Time》 |文本预训练模型 |
| 81  | 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》 |文本预训练模型 |
| 82  | 《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》 |文本预训练模型 |
| 83  | 《THE COST OF TRAINING NLP MODELS A CONCISE OVERVIEW》 |文本预训练模型 |
| 84  | 《SpanBERT: Improving Pre-training by Representing and Predicting Spans》 |文本预训练模型 |
| 85  | 《Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks》 |文本预训练模型 |
| 86  | 《Pre-Training with Whole Word Masking for Chinese BERT》 |文本预训练模型 |
| 87  | 《NEZHA: Neural Contextualized Representation for Chinese Language Understanding》 |文本预训练模型 |
| 88  | 《ZEN: Pre-training Chinese (Z) Text Encoder Enhanced by N-gram Representations 》 |文本预训练模型 |
| 89  | 《Revisiting Pre-Trained Models for Chinese Natural Language Processing》 |文本预训练模型 |
| 90  | 《Efficient Estimation of Word Representations in Vector Space》 |文本向量化 |
| 91  | 《Distributed Representations of Words and Phrases and their Compositionality》 |文本向量化 |
| 92  | 《word2vec Parameter Learning Explained》 |文本向量化 |
| 93  | 《word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method》 |文本向量化 |
| 94  | 《Enriching Word Vectors with Subword Information》 |文本向量化 |
| 95  | 《Bag of Tricks for Efficient Text Classification》 |文本向量化 |
| 96  | 《A Neural Probabilistic Language Model》 |文本向量化 |
| 97  | 《Universal Sentence Encoder for English》 |文本向量化 |
| 98  | 《Distributed Representations of Sentences and Documents》 |文本向量化 |
| 99  | 《A Convolutional Neural Network for Modelling Sentences》 |文本向量化 |
| 100  | 《Multi-Criteria Chinese Word Segmentation with Transformer》 |中文分词 |
| 101  | 《Multi-Grained Chinese Word Segmentation》 |中文分词 |
| 102  | 《基于文档主题结构的关键词抽取方法研究》 |关键词提取 |
| 103  | 《Topic-Aware Neural Keyphrase Generation for Social Media Language》 |关键词提取 |
| 104  | 《Context-Aware Document Term Weighting for Ad-Hoc Search》 |关键词提取 |
| 105  | 《Title-Guided Encoding for Keyphrase Generation》 |关键词提取 |
| 106  | 《深度文本匹配综述》 |文本相似度计算 |
| 107  | 《A Deep Look into Neural Ranking Models for Information Retrieval》 |文本相似度计算 |
| 108  | 《Deep Learning Based Text Classification: A Comprehensive Review》 |文本分类 |
| 109  | 《Hierarchical Multi-Label Classification Networks》 |文本分类 |
| 110  | 《Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN》 |文本分类 |
| 111  | 《Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification》 |文本分类 |
| 112  | 《Learning Deep Latent Spaces for Multi-Label Classification》 |文本分类 |
| 113  | 《Joint Embedding of Words and Labels for Text Classification》 |文本分类 |
| 114  | 《Enhancing Local Feature Extraction with Global Representation for Neural Text Classification》 |文本分类 |
| 115  | 《Description Based Text Classification with Reinforcement Learning》 |文本分类 |
| 116  | 《Document Modeling with Gated Recurrent Neural Network for Sentiment Classification》 |文本分类 |
| 117  | 《Neural Attentive Bag-of-Entities Model for Text Classification》 |文本分类 |
| 118  | 《An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis》 |情感分析 |
| 119  | 《Deep Learning for Aspect-Level Sentiment Classification: Survey, Vision, and Challenges》 |情感分析 |
| 120  | 《Deep learning for sentiment analysis: A survey》 |情感分析 |
| 121  | 《Hierarchical Topic Models and the Nested Chinese Restaurant Process》 |主题模型 |
| 122  | 《Topic Modeling for Personalized Recommendation of Volatile Items》 |主题模型 |
| 123  | 《Topic Memory Networks for Short Text Classification》 |主题模型 |
| 124  | 《Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec》 |主题模型 |
| 125  | 《Discovering Discrete Latent Topics with Neural Variational Inference》 |主题模型 |
| 126  | 《Federated Topic Modeling》 |主题模型 |
| 127  | 《ATM:Adversarial-neural Topic Model》 |主题模型 |
| 128  | 《Neural Machine Reading Comprehension: Methods and Trends》 |阅读理解 |
| 129  | 《Neural Reading Comprehension And Beyond》 |阅读理解 |
| 130  | 《Get To The Point: Summarization with Pointer-Generator Networks》 |文本改写&生成 |
| 131  | 《Rigid Formats Controlled Text Generation》 |文本改写&生成 |
| 132  | 《CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling》 |文本改写&生成 |
| 133  | 《UNSUPERVISED PARAPHRASE GENERATION USING PRE-TRAINED LANGUAGE MODELS》 |文本改写&生成 |
| 134  | 《FASPell: A Fast, Adaptable, Simple, Powerful Chinese Spell Checker Based On DAE-Decoder Paradigm》 |文本检错&纠错 |
| 135  | 《文本纠错的探索与实践》平安人寿：陈乐清 |文本检错&纠错  |
| 136  | 《Get To The Point: Summarization with Pointer-Generator Network》 |摘要生成 |
| 137  | 《Encode, Tag, Realize: High-Precision Text Editing》 |摘要生成 |
| 138  | 《Multi-Source Pointer Network for Product Title Summarization》 |摘要生成 |
| 139  | 《Abstractive Summarization: A Survey of the State of the Art》 |摘要生成 |
| 140  | 《Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products》 |摘要生成 |
| 141  | 《3D Convolutional Neural Networks for Human Action Recognition》 |视频分类 |
| 142  | 《Aggregating local descriptors into a compact image representation》 |图片视频向量化（特征提取） |
| 143  | 《NetVLAD: CNN architecture for weakly supervised place recognition》 |NULL |
| 144  | 《NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification》 |NULL |
| 145  | 《First Order Motion Model for Image Animation》 |视觉应用 |
| 146  | 《CNN ARCHITECTURES FOR LARGE-SCALE AUDIO CLASSIFICATION》 |智能语音 |
| 147  | 《Multimodal Machine Learning: A Survey and Taxonomy》 |多模态 |
| 148  | 《ERNIE-VIL: KNOWLEDGE ENHANCED VISION-LANGUAGE REPRESENTATIONS THROUGH SCENE GRAPH》 |NULL |
| 149  | 《VideoBERT: A Joint Model for Video and Language Representation Learning》 |NULL |
| 150  | 《VistaNet: Visual Aspect Attention Network for Multimodal Sentiment Analysis》 |NULL |
| 151  | 《Supervised Multimodal Bitransformers for Classifying Images and Text》 |NULL |
| 152  | 《Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks》 |NULL |
| 153  | 《Tensor fusion network for multimodal sentiment analysis》 |NULL |
| 154  | 《NetVLAD: CNN architecture for weakly supervised place recognition》 |序列特征处理 |
| 155  | 《Neural Approaches to Conversational AI Question Answering, Task-Oriented Dialogues and Social Chatbots》 |务型对话 |
| 156  | 《The Dialog State Tracking Challenge Series: A Review》 |NULL |
| 157  | 《MACHINE LEARNING FOR DIALOG STATE TRACKING:A REVIEW》 |NULL |
| 158  | 《Deep Neural Network Approach for the Dialog State Tracking Challenge》 |NULL |
| 159  | 《BERT for Joint Intent Classifification and Slot Filling》 |NULL |
| 160  | 《Task-Oriented Dialogue as Dataflow Synthesis》 |NULL |
| 161  | 《Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems》 |NULL |
| 162  | 《FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance》 |QA对话 |
| 163  | 《PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable》 |闲聊对话&开放域问答 |
| 164  | 《PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning》 |NULL |
| 165  | 《Improving Multi-turn Dialogue Modelling with Utterance ReWriter》 |NULL |
| 166  | 《Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models》 |NULL |
| 167  | 《A Neural Conversational Model》 |NULL |
| 168  | 《The Design and Implementation of XiaoIce, an Empathetic Social Chatbot》 |NULL |
| 169  | 《Challenges in Building Intelligent Open-domain Dialog Systems》 |NULL |
| 170  | 《Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs》 |图谱问答 |
| 171  | 《Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment》 |对话评估&强化学习 |
| 172  | 《A Survey of Techniques for Constructing Chinese Knowledge Graphs and Their Applications 》 |知识图谱综述 |
| 173  | 《A Survey on Deep Learning for Named Entity Recognition》 |命名实体识别 |
| 174  | 《Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data》 |NULL |
| 175  | 《Bidirectional LSTM-CRF Models for Sequence Tagging》 |NULL |
| 176  | 《Neural Architectures for Named Entity Recognition》 |NULL |
| 177  | 《Chinese NER Using Lattice LSTM》 |NULL |
| 178  | 《Simplify the Usage of Lexicon in Chinese NER》 |NULL |
| 179  | 《Learning Named Entity Tagger using Domain-Specific Dictionary》 |NULL |
| 180  | 《Neural Architectures for Fine-grained Entity Type Classification》 |NULL |
| 181  | 《An attentive fine-grained entity typing model with latent type representation》 |NULL |
| 182  | 《Hierarchical Entity Typing via Multi-level Learning to Rank》 |NULL |
| 183  | 《Deep Exhaustive Model for Nested Named Entity Recognition》 |NULL |
| 184  | 《A Boundary-aware Neural Model for Nested Named Entity Recognition》 |NULL |
| 185  | 《TENER: Adapting Transformer Encoder for Named Entity Recognition》 |NULL |
| 186  | 《Joint Learning of Named Entity Recognition and Entity Linking》 |NULL |
| 187  | 《Multilingual Named Entity Recognition Using Pretrained Embeddings, Attention Mechanism and NCRF》 |NULL |
| 188  | 《Hierarchically-Refined Label Attention Network for Sequence Labeling》 |NULL |
| 189  | 《Fast and Accurate Entity Recognition with Iterated Dilated Convolutions》 |NULL |
| 190  | 《Learning to Bootstrap for Entity Set Expansion》 |实体扩展 |
| 191  | 《Classifying Relations by Ranking with Convolutional Neural Networks》 |关系抽取 |
| 192  | 《End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures》 |NULL |
| 193  | 《Joint entity and relation extraction based on a hybrid neural network》 |NULL |
| 194  | 《Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme》 |NULL |
| 195  | 《FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation》 |NULL |
| 196  | 《Joint Extraction of Entities and Relations with a Hierarchical Multi-task Tagging Model》 |NULL |
| 197  | 《A Hierarchical Framework for Relation Extraction with Reinforcement Learning》 |NULL |
| 198  | 《Pre-training of Deep Contextualized Embeddings of Words and Entities for Named Entity Disambiguation》 |实体链接（消歧&归一化） |
| 199  | 《Multimodal Attribute Extraction》 |属性抽取 |
| 200  | 《Deep Learning based Recommender System: A Survey and New Perspectives》 |推荐 |
| NULL  | 《推荐系统调研报告及综述》 |NULL |
| NULL  | 《Deep Neural Networks for YouTube Recommendations》 |NULL |
| NULL  | 《Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features》 |NULL |
| NULL  | 《Deep & Cross Network for Ad Click Predictions》 |NULL |
| NULL  | 《MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu’s Sponsored Search》 |NULL |
| NULL  | 《Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations 》 |NULL |
| NULL  | 《bipartite graph neural networks for efficient node representation learning》 |NULL |
| NULL  | 《Wide & Deep Learning for Recommender Systems》 |NULL |
| NULL  | 《SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS》 |NULL |
| NULL  | 《Semantic search on text and knowledge bases》 |搜索 |
| NULL  | 《A Deep Generative Approach to Search Extrapolation and Recommendation》 |NULL |
| NULL  | 《A User-Centered Concept Mining System for Query and Document Understanding at Tencent》 |标签体系&标签扩展&兴趣标签&概念标签 |
| NULL  | 《GIANT: Scalable Creation of a Web-scale Ontology》 |NULL |
| NULL  | 《Meta-Learning for Qery Conceptualization at Web Scale》 |NULL |
| NULL  | 《AliCoCo: Alibaba E-commerce Cognitive Concept Net》 |NULL |
| NULL  | 《Octet: Online Catalog Taxonomy Enrichment with Self-Supervision》 |NULL |
| NULL  | 《Scaling Up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Title》 |标签生成 |
| NULL  | 《Microblog Hashtag Generation via Encoding Conversation Contexts》 |NULL |
| NULL  | 《Cognitive Representation Learning of Self-Media Online Article Quality》 |文本审核&质量 |
| NULL  | 《Spam Review Detection with Graph Convolutional Networks》 |NULL |
| NULL  | 《Abusive Language Detection with Graph Convolutional Networks》 |NULL |
| NULL  | 《Weak Supervision for Fake News Detection via Reinforcement Learning》 |NULL |
| NULL  | 《Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model》 |评论生成&改写 |
| NULL  | 《Deep Active Learning for Short-Text Classification》 |主动学习 |
| NULL  | 《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》 |数据增强 |
| NULL  | 《UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING》 |半监督学习 |
| NULL  | 《Data Programming:Creating Large Training Sets, Quickly》 |NULL |
| NULL  | 《Rethinking the Value of Labels for Improving Class-Imbalanced Learning》 |NULL |
| NULL  | 《Extreme Multi-label Loss Functions for Recommendation,Tagging, Ranking & Other Missing Label Applications》 |多标签残缺 |
| NULL  | 《基于不完整标签信息的多标签分类问题研究》 |NULL |
| NULL  | 《Dice loss for data-imbalance NLP tasks》 |样本不均衡 |
| NULL  | 《Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting》 |NULL |
| NULL  | 《ehensive Introduction to Label Noise》 |噪声标签 |
| NULL  | 《Classification in the Presence of Label Noise: a Survey》 |NULL |
| NULL  | 《Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey》 |NULL |
| NULL  | 《Confident Learning: Estimating Uncertainty in Dataset Labels》 |NULL |
| NULL  | 《Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels》 |NULL |
| NULL  | 《Learning with Noisy Label-深度学习廉价落地》 |NULL |
| NULL  | 《TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing》 |知识蒸馏 |
| NULL  | 《Mixed Precision Training》 |量化加速 |
| NULL  | 《Why Should I Trust You? : Explaning the Predictions of Any Classifier》 |模型可解释性 |
| NULL  | 《Analyzing and Interpreting Neural Networks for NLP: A Report on the First BlackboxNLP Workshop》 |NULL |
| NULL  | 《NeuralClassifier: An Open-source Neural Hierarchical Multi-label Text Classification Toolkit》 |算法框架&工具 |
| NULL  | 《Familia: A Configurable Topic Modeling Framework for Industrial Text Engineering》 |NULL |
| NULL  | 《LightLDA: Big Topic Models on Modest Computer Clusters》 |NULL |
| NULL  | 《DeepPavlov: Open-Source Library for Dialogue Systems》 |NULL |
| NULL  | 《AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models》 |NULL |
| NULL  | 《Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering》 |其他 |
| NULL  | 《Analysis Methods in Neural Language Processing: A Survey》 |NULL |
| NULL  | 《Deep Double Descent: Where Bigger Models and More Data Hurt》 |NULL |
| NULL  | 《中文信息处理发展报告》 |NULL |
| NULL  | 《Modern Deep Learning Techniques Applied to Natural Language Processing》 |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
