|编号  |  名称   | 目录  |
|  ----  | ----  |----  | 
| NULL  | 《A State-of-the-Art Survey on Deep Learning Theory and Architectures》 |机器学习&深度学习综述 |
| NULL  | 《Deep Learning》 |NULL |
| NULL  | 《learning internal representation by error propagation》 |DNN |
| NULL  | 《LONG SHORT-TERM MEMORY》 |RNN |
| NULL  | 《Gradient-Based Learning Applied to Document Recognition》 |CNN |
| NULL  | 《Attention Is All You Need》 |Transformer |
| NULL  | 《Star-Transformer》 |NULL |
| NULL  | 《Transformer-XL: Language Modeling with Longer-Term Dependency》 |NULL |
| NULL  | 《An Introductory Survey on Attention Mechanisms in NLP Problems》 |Attention机制&指针网络&记忆网络&深度图灵机 |
| NULL  | 《Pointer Networks》 |NULL |
| NULL  | 《Attention-Based LSTM for Psychological Stress Detection from Spoken Language Using Distant Supervision》 |NULL |
| NULL  | 《End-To-End Memory Networks》 |NULL |
| NULL  | 《Neural Turing Machines》 |NULL |
| NULL  | 《Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey》 |对抗攻击 |
| NULL  | 《TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP》 |NULL |
| NULL  | 《Tackling Graphical NLP problems with Graph Recurrent Networks》 |图网络 |
| NULL  | 《阿尔伯塔大学博士毕业论文：基于图结构的自然语言处理》 |NULL |
| NULL  | 《Convolutional Networks on Graphs for Learning Molecular Fingerprints》 |NULL |
| NULL  | 《SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS》 |NULL |
| NULL  | 《The graph neural network model》 |NULL |
| NULL  | 《Deepwalk: Online learning of social representations》 |NULL |
| NULL  | 《SkipGram model on the generated random walks》 |NULL |
| NULL  | 《HOW POWERFUL ARE GRAPH NEURAL NETWORKS?》 |NULL |
| NULL  | 《Learning Convolutional Neural Networks for Graphs》 |NULL |
| NULL  | 《Graph Neural Networks:A Review of Methods and Applications》 |NULL |
| NULL  | 《A Comprehensive Survey on Graph Neural Networks》 |NULL |
| NULL  | 《Deep Learning on Graphs: A Survey》 |NULL |
| NULL  | 《LINE: Large-scale Information Network Embedding》 |NULL |
| NULL  | 《Graph Attention Networks》 |NULL |
| NULL  | 《struc2vec: Learning Node Representations from Structural Identity》 |NULL |
| NULL  | 《GraphSAINT: Graph Sampling Based Inductive Learning Method》 |NULL |
| NULL  | 《GraphSage: Graph Sampling Based Inductive Learning Method》 |NULL |
| NULL  | 《DROPEDGE: TOWARDS DEEP GRAPH CONVOLUTIONAL NETWORKS ON NODE CLASSIFICATION》 |NULL |
| NULL  | 《Evolution of Transfer Learning in Natural Language Processing》 |迁移学习 |
| NULL  | 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》 |NULL |
| NULL  | 《Domain-Adversarial Training of Neural Networks》 |领域适应 |
| NULL  | 《Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms》 |AutoML |
| NULL  | 《AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data》 |NULL |
| NULL  | 《A Survey on Causal Inference》 |因果推断 |
| NULL  | 《迈向第三代人工智能》 |常识AI |
| NULL  | 《DEEP LEARNING FOR SYSTEM 2 PROCESSING，YOSHUA BENGIO》 |NULL |
| NULL  | 《FROM SYSTEM 1 DEEP LEARNING TO SYSTEM 2 DEEP LEARNING,YOSHUA BENGIO》 |NULL |
| NULL  | 《Cognitive Graph for Multi-Hop Reading Comprehension at Scale》 |NULL |
| NULL  | 《Auto-Encoding Variational Bayes》 |贝叶斯神经网络 |
| NULL  | 《Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks》 |神经逻辑推理 |
| NULL  | 《Neural Logic Reasoning》 |NULL |
| NULL  | 《Recent Trends in Deep Learning Based Natural Language Processing》 |自然语言处理综述 |
| NULL  | 《Pre-trained Models for Natural Language Processing: A Survey》 |文本预训练模型 |
| NULL  | 《Semi-supervised Sequence Learning》 |NULL |
| NULL  | 《Learned in Translation: Contextualized Word Vectors》 |NULL |
| NULL  | 《Semi-supervised sequence tagging with bidirectional language models》 |NULL |
| NULL  | 《Universal Language Model Fine-tuning for Text Classification》 |NULL |
| NULL  | 《Deep contextualized word representation》 |NULL |
| NULL  | 《Improving Language Understanding by Generative Pre-Training》 |NULL |
| NULL  | 《Language Models are Unsupervised Multitask Learners》 |NULL |
| NULL  | 《Language Models are Few-Shot Learners》 |NULL |
| NULL  | 《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》 |NULL |
| NULL  | 《Cross-lingual Language Model Pretraining》 |NULL |
| NULL  | 《MASS: Masked Sequence to Sequence Pre-training for Language Generation》 |NULL |
| NULL  | 《Unified Language Model Pre-training for Natural Language Understanding and Generation》 |NULL |
| NULL  | 《UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training》 |NULL |
| NULL  | 《MPNet: Masked and Permuted Pre-training for Language Understanding》 |NULL |
| NULL  | 《RoBERTa: A Robustly Optimized BERT Pretraining Approach》 |NULL |
| NULL  | 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》 |NULL |
| NULL  | 《Pre-Training with Whole Word Masking for Chinese BERT》 |NULL |
| NULL  | 《ERNIE: Enhanced Language Representation with Informative Entities》 |NULL |
| NULL  | 《ERNIE: Enhanced Representation through Knowledge Integration》 |NULL |
| NULL  | 《ERNIE 2.0: A CONTINUAL PRE-TRAINING FRAMEWORK FORLANGUAGE》 |NULL |
| NULL  | 《ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators》 |NULL |
| NULL  | 《K-BERT: Enabling Language Representation with Knowledge Graph》 |NULL |
| NULL  | 《StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding》 |NULL |
| NULL  | 《Semantics-aware BERT for Language Understanding》 |NULL |
| NULL  | 《XLNet: Generalized Autoregressive Pretraining for Language Understanding》 |NULL |
| NULL  | 《Longformer: The Long-Document Transformer》 |NULL |
| NULL  | 《Big Bird: Transformers for Longer Sequences》 |NULL |
| NULL  | 《Well-Read Students Learn Better: On the Importance of Pre-training Compact Models》 |NULL |
| NULL  | 《ALBERT: A Lite BERT for Self-supervised Learning of Language Representations》 |NULL |
| NULL  | 《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》 |NULL |
| NULL  | 《TinyBERT: Distilling BERT for Natural Language Understanding》 |NULL |
| NULL  | 《FastBERT: a Self-distilling BERT with Adaptive Inference Time》 |NULL |
| NULL  | 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》 |NULL |
| NULL  | 《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》 |NULL |
| NULL  | 《THE COST OF TRAINING NLP MODELS A CONCISE OVERVIEW》 |NULL |
| NULL  | 《SpanBERT: Improving Pre-training by Representing and Predicting Spans》 |NULL |
| NULL  | 《Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks》 |NULL |
| NULL  | 《Pre-Training with Whole Word Masking for Chinese BERT》 |NULL |
| NULL  | 《NEZHA: Neural Contextualized Representation for Chinese Language Understanding》 |NULL |
| NULL  | 《ZEN: Pre-training Chinese (Z) Text Encoder Enhanced by N-gram Representations 》 |NULL |
| NULL  | 《Revisiting Pre-Trained Models for Chinese Natural Language Processing》 |NULL |
| NULL  | NULL |文本向量化 |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
| NULL  | NULL |NULL |
