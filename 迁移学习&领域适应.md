|名称  |  来源   | 说明  |状态   | 备注  |
|  ----  | ----  |----  | ----  |----  |
| 《Evolution of Transfer Learning in Natural Language Processing》| arXiv2019|NLP基础模型汇总（论文质量有些粗糙，但作为资源索引和模型梳理还不错）：<br/>1 Vanilla RNNs;<br/>2 Long Short Term Memory;<br/>3 Gated Recurrent Units;<br/>4 Average SGD Weight Dropped(AWD)-LSTM;<br/>5 Seq2Seq Architecture;<br/>6 Attention Mechanism;<br/>7 Transformer;<br/>8 ULMFIT;<br/>9 Embeddings from Language Models(ELMo)<br/>10 OpenAI Transformer<br/>11 Bidirectional Encoder Represenation from Transformers(BERT)<br/>12 Universal sentence encoder<br/>13 Transformer-XL<br/>14 XLNet|NULL |LSTM参考：http://pages.cs.wisc.edu/~shavlik/cs638/lectureNotes/Long%20Short-Term%20Memory%20Networks.pdf|
| 《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》  | Arxiv2019 |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
| NULL  | NULL |NULL |NULL |NULL |
